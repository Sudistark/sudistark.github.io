<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Bug Hunting Journey of 2019 | Your awesome title</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Bug Hunting Journey of 2019" />
<meta name="author" content="GitHub User" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Heyyy Everyoneee, I hope you all are doing good, this year is about to end. So I thought I should share a last writeup about some of the bugs which I have found this year.This is going to be a little long.I have been working on this for the last few days ,I hope you will like it. I will be starting from the xss bugs which I found in a Private Program websites,I decided to look into the android apps in hope of finding some new endpoints that might be vulneable. Here we go…. I never checked their android apps before so I decided to give it a try.I downloaded the android apps from app.evozi.com, after downloading them I used apktool to decompile them.Then I search for http/https endpoints. You can use grep to search for strings starting with https,http. I was suprised to see some new domains which I didn’t find earlier,tried putting the same vulnerable endpoints in which I have found xss earlier,result successful popup. But sad, someone already reported it, 11 days ago. You can use this tool for extracting endpoints/URLs from apk files. https://github.com/s0md3v/Diggy s0md3v/Diggy I didn’t knew about apktool,grep at first so I was doing it all manually. Extracting the apk file , then converting the classes.dex file into jar using dex2jar and then finally searching for http,https By seeing the above screenshot you could easily see that I was doing it the hard way(wrong way).Later on I started looking for an easy way and found s0md3v’s tool. Now moving on to Verizon Media I won’t be mentioning much more details about the bugs found in VerizonMedia program as they don’t allow disclosure of their bugs,I will be only talk about the process how I came across this bugs. I decided to start hunting on Verizon Program, I was just crazy to find a xss in yahoo subdomain. Fired up my burpsuite and I collected the subdomains from various subdoman enumeration tools like subfinder,assetfinder,findomain,etc . one by one I was opening the subdomains in my browser,I was monitoring them in burp (I was focused more on finding some strange looking subdomains) and also selecting all the inscope hosts and spidering them in hope of finding some more subdomains, following Jhaddix way (Checkout out his videos, if you haven’t). https://medium.com/media/98ee338878b766e261e8d39047ba76b8/href I continued following the same routine,well it was so boring and tedious but still I continued looking for a xss. One day I found a strange 3 level yahoo subdomain in Target area,I clicked on it all I can see there is only two endpoints but there were many parameters,so I started testing these parameters ,my input got reflected into the source page for eg : ‘“&gt;&lt;shirley&gt;as it is,I used the payload “&gt;&lt;img src=x onerror=alert(document.domain)&gt; to make the xss popup. Quickly I decided to report it.Later,saw many people getting duplicate of this report, felt sad for them :( .If I had found that xss after 1–2days I would have surely gotten duplicate too. I had created a python script by following some blog posts from google about how to open urls from a text file one by one in browser at that time,btw there is any easy to do this ,using this addon Open Multiple URLs it’s available for both Chrome and Firefox just supply it a list of urls and it will open them in your browser all at once. Moving on to the next bug, I started targetting Facebook just after few days I was done with Yahoo,I have already published an article about that subdomain takeover so there isn’t much left to talk about. If you haven’t checked it out already yet,just open the below link,I have talked about in detail how I was able to find this domain: How Recon helped me to to find a Facebook domain takeover I followed the same methodology and was able to find two more subdomain takeovers this time in a Private BugBounty Program. Checked the whois records of some company’s owned website,compared the results and then used similar terms to find more domains.Used webscreenshot, then I started going through them to look for interesting stuffs -&gt; found 2 unclaimed github pages. Let’s now talk about some Privilege Escalation Bugs: We can use some automation here with the help of Burp addons like Autorize,Autorepeater.You can follow these two links here : Escalating Privileges like a Pro (gauravnarwani97) https://medium.com/media/12821f47199bf52aac3ea925447af0dd/href Watch this video, it will guide you how to use Autorepeater and Autorize.(@stokfredrik &amp; @Fisher) Picked up some private programs from hackerone which have websites that have user management system where I can see they have setup different user roles, so I can check privilege escalation issues. For a quick check,I like to start by creating a new account with no permission (absolutely nothing).Then after configuring autorize, with the inscope target.(You can checkout the above mentioned video and articles they include the steps also how to setup autorize for testing privilege escalation issues). Login into the high Privilege user account and start going through every endpoint on that web application as much possible, and after some time looking at the autorize tab to see if it has find something. The extension autorize automatically repeats every request with the session of the low privileged user and detects authorization vulnerabilities.(https://portswigger.net/bappstore/f9bbac8c4acf4aefa4d7dc92a991af2f) I was able to found 2 privsec issues in one program and 1 issue in another.One of them which was a low Privilege user was able to view the details of other users in an organization. Screenshot of the request and response.(Private Program) I really like the website which have already having a check box like table in their doc, so that the users can very easily identify what all permisison a specific type of user is able to perform. Well if there isn’t already a table like that, make it yourself it will help you while testing. Result:Btw, Got duplicate for all three of them :( Again back to Verizon Media This is going to be about the information disclosure bug,basically I found an endpoint which was disclosing the public/private RSA key just like this [privateKey] =&gt; — — — BEGIN RSA PRIVATE KEY — — ………………………Redacted……………………………… — — — END RSA PRIVATE KEY — — — I was applying the same methodology which I used in Facebook ,in case of Facebook it was about 4k unique domains.But here in case of Verizon Media, there were f**king more than 10k+ domains just from using domain-admin@oath.com as the search term in https://tools.whoisxmlapi.com/reverse-whois-search Another 10k+ domains if I use Oath Inc as the search term.Although I followed the same way like before,collecting all the subdomains -&gt; screenshot. Going through them but here if we talk about Yahoo’s site many of them redirects to Yahoo search, Will be right back…,Sorry, the page you requested was not found,etc.It’s going to take a lot of time if I followed the old way. I was looking for some other new methodology this time,started reading recon writeups,searching on twitter. Got to know about some of the @tomnomnom’s awesome tools-&gt; httprobe,meg,assetfinder but still I needed to figure out how should I use them. At first I watched https://medium.com/media/580a30542471d90280da28de8895a3c1/href where @tomnomnom talked about his recon techniques, he mentioned about how we can use meg to find bugs Path-based xss, Open redirects,CRLF injection,CORS misconfigurations.There were some more videos also of him which I have watched you can find them on his channel and one on Nahamsec stream. Like this one which he did with @stokfredrik https://medium.com/media/cbcaa1b1116300945298186858e71de5/href I used httprobe to resolve all the domains which I found from https://tools.whoisxmlapi.com/reverse-whois-search and saved the output to a file. cat domains|httprobe -c 100|tee hosts Then with the help of sed ,removed http[s]:// part from the contents of the hosts file and used sort -u command to removed the duplicate ones pipe the output to assetfinder which will find subdomains for us and after . cat hosts | sed ‘s/^http\(\|s\):\/\///g’\ | sort -u | assetfinder —subs-only |tee subdomains After i got the subdomains again used httprobe and after that I used whatweb (got to know about this tool from Nahamsec stream,Shubham Shah mentioned about it there). WhatWeb helps in recognisesing web technologies used by a particular website,here’s the tool in action Using grep , I took out only those websites which were built upon PHP cat whatweb-result|grep “PHP” Saved all the PHP websites in one file, used webscreenshot to took screenshot of them and also made bash loop for dirsearch (for bruteforcing directories). for i in `cat target`;do dirsearch -u $i -e *done; From the screenshot, I found a website which was having blank page, I checked the source code and found few php endpoints there.Opened them,but there wasn’t anything.So I checked the dirsearch scan results for this website , interesting there was 200 ok response on /.svn/all-wcprops Quickly opened it and I saw there many php files name with their full path.Like this there were many php endpoints. /var/www/test.php …………../db.php I made a wordlist with all these php endpoints , stripping the path.Again used dirsearch on this website with this newly created wordlist. When I opened one of this php endpoint, it was disclosing the private/public Rsa key.I quickly reported it.I wasn’t sure if it would have get accepted or not as I having a doubt if this website was in the scope or not. It was fixed within 1–2 days they shutdown the website, and turns out that the website was ou of scope but still I was awarded with a bounty of $2500, which is really great of them. For dirbruteforcing I have switched to ffuf now,give it a try you will love it :) Thankyou so much,if you have read it till the end really appreciated your time.Godluck for 2020 :) Sya Everyonee!!!" />
<meta property="og:description" content="Heyyy Everyoneee, I hope you all are doing good, this year is about to end. So I thought I should share a last writeup about some of the bugs which I have found this year.This is going to be a little long.I have been working on this for the last few days ,I hope you will like it. I will be starting from the xss bugs which I found in a Private Program websites,I decided to look into the android apps in hope of finding some new endpoints that might be vulneable. Here we go…. I never checked their android apps before so I decided to give it a try.I downloaded the android apps from app.evozi.com, after downloading them I used apktool to decompile them.Then I search for http/https endpoints. You can use grep to search for strings starting with https,http. I was suprised to see some new domains which I didn’t find earlier,tried putting the same vulnerable endpoints in which I have found xss earlier,result successful popup. But sad, someone already reported it, 11 days ago. You can use this tool for extracting endpoints/URLs from apk files. https://github.com/s0md3v/Diggy s0md3v/Diggy I didn’t knew about apktool,grep at first so I was doing it all manually. Extracting the apk file , then converting the classes.dex file into jar using dex2jar and then finally searching for http,https By seeing the above screenshot you could easily see that I was doing it the hard way(wrong way).Later on I started looking for an easy way and found s0md3v’s tool. Now moving on to Verizon Media I won’t be mentioning much more details about the bugs found in VerizonMedia program as they don’t allow disclosure of their bugs,I will be only talk about the process how I came across this bugs. I decided to start hunting on Verizon Program, I was just crazy to find a xss in yahoo subdomain. Fired up my burpsuite and I collected the subdomains from various subdoman enumeration tools like subfinder,assetfinder,findomain,etc . one by one I was opening the subdomains in my browser,I was monitoring them in burp (I was focused more on finding some strange looking subdomains) and also selecting all the inscope hosts and spidering them in hope of finding some more subdomains, following Jhaddix way (Checkout out his videos, if you haven’t). https://medium.com/media/98ee338878b766e261e8d39047ba76b8/href I continued following the same routine,well it was so boring and tedious but still I continued looking for a xss. One day I found a strange 3 level yahoo subdomain in Target area,I clicked on it all I can see there is only two endpoints but there were many parameters,so I started testing these parameters ,my input got reflected into the source page for eg : ‘“&gt;&lt;shirley&gt;as it is,I used the payload “&gt;&lt;img src=x onerror=alert(document.domain)&gt; to make the xss popup. Quickly I decided to report it.Later,saw many people getting duplicate of this report, felt sad for them :( .If I had found that xss after 1–2days I would have surely gotten duplicate too. I had created a python script by following some blog posts from google about how to open urls from a text file one by one in browser at that time,btw there is any easy to do this ,using this addon Open Multiple URLs it’s available for both Chrome and Firefox just supply it a list of urls and it will open them in your browser all at once. Moving on to the next bug, I started targetting Facebook just after few days I was done with Yahoo,I have already published an article about that subdomain takeover so there isn’t much left to talk about. If you haven’t checked it out already yet,just open the below link,I have talked about in detail how I was able to find this domain: How Recon helped me to to find a Facebook domain takeover I followed the same methodology and was able to find two more subdomain takeovers this time in a Private BugBounty Program. Checked the whois records of some company’s owned website,compared the results and then used similar terms to find more domains.Used webscreenshot, then I started going through them to look for interesting stuffs -&gt; found 2 unclaimed github pages. Let’s now talk about some Privilege Escalation Bugs: We can use some automation here with the help of Burp addons like Autorize,Autorepeater.You can follow these two links here : Escalating Privileges like a Pro (gauravnarwani97) https://medium.com/media/12821f47199bf52aac3ea925447af0dd/href Watch this video, it will guide you how to use Autorepeater and Autorize.(@stokfredrik &amp; @Fisher) Picked up some private programs from hackerone which have websites that have user management system where I can see they have setup different user roles, so I can check privilege escalation issues. For a quick check,I like to start by creating a new account with no permission (absolutely nothing).Then after configuring autorize, with the inscope target.(You can checkout the above mentioned video and articles they include the steps also how to setup autorize for testing privilege escalation issues). Login into the high Privilege user account and start going through every endpoint on that web application as much possible, and after some time looking at the autorize tab to see if it has find something. The extension autorize automatically repeats every request with the session of the low privileged user and detects authorization vulnerabilities.(https://portswigger.net/bappstore/f9bbac8c4acf4aefa4d7dc92a991af2f) I was able to found 2 privsec issues in one program and 1 issue in another.One of them which was a low Privilege user was able to view the details of other users in an organization. Screenshot of the request and response.(Private Program) I really like the website which have already having a check box like table in their doc, so that the users can very easily identify what all permisison a specific type of user is able to perform. Well if there isn’t already a table like that, make it yourself it will help you while testing. Result:Btw, Got duplicate for all three of them :( Again back to Verizon Media This is going to be about the information disclosure bug,basically I found an endpoint which was disclosing the public/private RSA key just like this [privateKey] =&gt; — — — BEGIN RSA PRIVATE KEY — — ………………………Redacted……………………………… — — — END RSA PRIVATE KEY — — — I was applying the same methodology which I used in Facebook ,in case of Facebook it was about 4k unique domains.But here in case of Verizon Media, there were f**king more than 10k+ domains just from using domain-admin@oath.com as the search term in https://tools.whoisxmlapi.com/reverse-whois-search Another 10k+ domains if I use Oath Inc as the search term.Although I followed the same way like before,collecting all the subdomains -&gt; screenshot. Going through them but here if we talk about Yahoo’s site many of them redirects to Yahoo search, Will be right back…,Sorry, the page you requested was not found,etc.It’s going to take a lot of time if I followed the old way. I was looking for some other new methodology this time,started reading recon writeups,searching on twitter. Got to know about some of the @tomnomnom’s awesome tools-&gt; httprobe,meg,assetfinder but still I needed to figure out how should I use them. At first I watched https://medium.com/media/580a30542471d90280da28de8895a3c1/href where @tomnomnom talked about his recon techniques, he mentioned about how we can use meg to find bugs Path-based xss, Open redirects,CRLF injection,CORS misconfigurations.There were some more videos also of him which I have watched you can find them on his channel and one on Nahamsec stream. Like this one which he did with @stokfredrik https://medium.com/media/cbcaa1b1116300945298186858e71de5/href I used httprobe to resolve all the domains which I found from https://tools.whoisxmlapi.com/reverse-whois-search and saved the output to a file. cat domains|httprobe -c 100|tee hosts Then with the help of sed ,removed http[s]:// part from the contents of the hosts file and used sort -u command to removed the duplicate ones pipe the output to assetfinder which will find subdomains for us and after . cat hosts | sed ‘s/^http\(\|s\):\/\///g’\ | sort -u | assetfinder —subs-only |tee subdomains After i got the subdomains again used httprobe and after that I used whatweb (got to know about this tool from Nahamsec stream,Shubham Shah mentioned about it there). WhatWeb helps in recognisesing web technologies used by a particular website,here’s the tool in action Using grep , I took out only those websites which were built upon PHP cat whatweb-result|grep “PHP” Saved all the PHP websites in one file, used webscreenshot to took screenshot of them and also made bash loop for dirsearch (for bruteforcing directories). for i in `cat target`;do dirsearch -u $i -e *done; From the screenshot, I found a website which was having blank page, I checked the source code and found few php endpoints there.Opened them,but there wasn’t anything.So I checked the dirsearch scan results for this website , interesting there was 200 ok response on /.svn/all-wcprops Quickly opened it and I saw there many php files name with their full path.Like this there were many php endpoints. /var/www/test.php …………../db.php I made a wordlist with all these php endpoints , stripping the path.Again used dirsearch on this website with this newly created wordlist. When I opened one of this php endpoint, it was disclosing the private/public Rsa key.I quickly reported it.I wasn’t sure if it would have get accepted or not as I having a doubt if this website was in the scope or not. It was fixed within 1–2 days they shutdown the website, and turns out that the website was ou of scope but still I was awarded with a bounty of $2500, which is really great of them. For dirbruteforcing I have switched to ffuf now,give it a try you will love it :) Thankyou so much,if you have read it till the end really appreciated your time.Godluck for 2020 :) Sya Everyonee!!!" />
<link rel="canonical" href="http://localhost:4000/2019/12/31/bug-hunting-journey-of-2019.html" />
<meta property="og:url" content="http://localhost:4000/2019/12/31/bug-hunting-journey-of-2019.html" />
<meta property="og:site_name" content="Your awesome title" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-12-31T00:00:00+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Bug Hunting Journey of 2019" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"GitHub User"},"dateModified":"2019-12-31T00:00:00+05:30","datePublished":"2019-12-31T00:00:00+05:30","description":"Heyyy Everyoneee, I hope you all are doing good, this year is about to end. So I thought I should share a last writeup about some of the bugs which I have found this year.This is going to be a little long.I have been working on this for the last few days ,I hope you will like it. I will be starting from the xss bugs which I found in a Private Program websites,I decided to look into the android apps in hope of finding some new endpoints that might be vulneable. Here we go…. I never checked their android apps before so I decided to give it a try.I downloaded the android apps from app.evozi.com, after downloading them I used apktool to decompile them.Then I search for http/https endpoints. You can use grep to search for strings starting with https,http. I was suprised to see some new domains which I didn’t find earlier,tried putting the same vulnerable endpoints in which I have found xss earlier,result successful popup. But sad, someone already reported it, 11 days ago. You can use this tool for extracting endpoints/URLs from apk files. https://github.com/s0md3v/Diggy s0md3v/Diggy I didn’t knew about apktool,grep at first so I was doing it all manually. Extracting the apk file , then converting the classes.dex file into jar using dex2jar and then finally searching for http,https By seeing the above screenshot you could easily see that I was doing it the hard way(wrong way).Later on I started looking for an easy way and found s0md3v’s tool. Now moving on to Verizon Media I won’t be mentioning much more details about the bugs found in VerizonMedia program as they don’t allow disclosure of their bugs,I will be only talk about the process how I came across this bugs. I decided to start hunting on Verizon Program, I was just crazy to find a xss in yahoo subdomain. Fired up my burpsuite and I collected the subdomains from various subdoman enumeration tools like subfinder,assetfinder,findomain,etc . one by one I was opening the subdomains in my browser,I was monitoring them in burp (I was focused more on finding some strange looking subdomains) and also selecting all the inscope hosts and spidering them in hope of finding some more subdomains, following Jhaddix way (Checkout out his videos, if you haven’t). https://medium.com/media/98ee338878b766e261e8d39047ba76b8/href I continued following the same routine,well it was so boring and tedious but still I continued looking for a xss. One day I found a strange 3 level yahoo subdomain in Target area,I clicked on it all I can see there is only two endpoints but there were many parameters,so I started testing these parameters ,my input got reflected into the source page for eg : ‘“&gt;&lt;shirley&gt;as it is,I used the payload “&gt;&lt;img src=x onerror=alert(document.domain)&gt; to make the xss popup. Quickly I decided to report it.Later,saw many people getting duplicate of this report, felt sad for them :( .If I had found that xss after 1–2days I would have surely gotten duplicate too. I had created a python script by following some blog posts from google about how to open urls from a text file one by one in browser at that time,btw there is any easy to do this ,using this addon Open Multiple URLs it’s available for both Chrome and Firefox just supply it a list of urls and it will open them in your browser all at once. Moving on to the next bug, I started targetting Facebook just after few days I was done with Yahoo,I have already published an article about that subdomain takeover so there isn’t much left to talk about. If you haven’t checked it out already yet,just open the below link,I have talked about in detail how I was able to find this domain: How Recon helped me to to find a Facebook domain takeover I followed the same methodology and was able to find two more subdomain takeovers this time in a Private BugBounty Program. Checked the whois records of some company’s owned website,compared the results and then used similar terms to find more domains.Used webscreenshot, then I started going through them to look for interesting stuffs -&gt; found 2 unclaimed github pages. Let’s now talk about some Privilege Escalation Bugs: We can use some automation here with the help of Burp addons like Autorize,Autorepeater.You can follow these two links here : Escalating Privileges like a Pro (gauravnarwani97) https://medium.com/media/12821f47199bf52aac3ea925447af0dd/href Watch this video, it will guide you how to use Autorepeater and Autorize.(@stokfredrik &amp; @Fisher) Picked up some private programs from hackerone which have websites that have user management system where I can see they have setup different user roles, so I can check privilege escalation issues. For a quick check,I like to start by creating a new account with no permission (absolutely nothing).Then after configuring autorize, with the inscope target.(You can checkout the above mentioned video and articles they include the steps also how to setup autorize for testing privilege escalation issues). Login into the high Privilege user account and start going through every endpoint on that web application as much possible, and after some time looking at the autorize tab to see if it has find something. The extension autorize automatically repeats every request with the session of the low privileged user and detects authorization vulnerabilities.(https://portswigger.net/bappstore/f9bbac8c4acf4aefa4d7dc92a991af2f) I was able to found 2 privsec issues in one program and 1 issue in another.One of them which was a low Privilege user was able to view the details of other users in an organization. Screenshot of the request and response.(Private Program) I really like the website which have already having a check box like table in their doc, so that the users can very easily identify what all permisison a specific type of user is able to perform. Well if there isn’t already a table like that, make it yourself it will help you while testing. Result:Btw, Got duplicate for all three of them :( Again back to Verizon Media This is going to be about the information disclosure bug,basically I found an endpoint which was disclosing the public/private RSA key just like this [privateKey] =&gt; — — — BEGIN RSA PRIVATE KEY — — ………………………Redacted……………………………… — — — END RSA PRIVATE KEY — — — I was applying the same methodology which I used in Facebook ,in case of Facebook it was about 4k unique domains.But here in case of Verizon Media, there were f**king more than 10k+ domains just from using domain-admin@oath.com as the search term in https://tools.whoisxmlapi.com/reverse-whois-search Another 10k+ domains if I use Oath Inc as the search term.Although I followed the same way like before,collecting all the subdomains -&gt; screenshot. Going through them but here if we talk about Yahoo’s site many of them redirects to Yahoo search, Will be right back…,Sorry, the page you requested was not found,etc.It’s going to take a lot of time if I followed the old way. I was looking for some other new methodology this time,started reading recon writeups,searching on twitter. Got to know about some of the @tomnomnom’s awesome tools-&gt; httprobe,meg,assetfinder but still I needed to figure out how should I use them. At first I watched https://medium.com/media/580a30542471d90280da28de8895a3c1/href where @tomnomnom talked about his recon techniques, he mentioned about how we can use meg to find bugs Path-based xss, Open redirects,CRLF injection,CORS misconfigurations.There were some more videos also of him which I have watched you can find them on his channel and one on Nahamsec stream. Like this one which he did with @stokfredrik https://medium.com/media/cbcaa1b1116300945298186858e71de5/href I used httprobe to resolve all the domains which I found from https://tools.whoisxmlapi.com/reverse-whois-search and saved the output to a file. cat domains|httprobe -c 100|tee hosts Then with the help of sed ,removed http[s]:// part from the contents of the hosts file and used sort -u command to removed the duplicate ones pipe the output to assetfinder which will find subdomains for us and after . cat hosts | sed ‘s/^http\\(\\|s\\):\\/\\///g’\\ | sort -u | assetfinder —subs-only |tee subdomains After i got the subdomains again used httprobe and after that I used whatweb (got to know about this tool from Nahamsec stream,Shubham Shah mentioned about it there). WhatWeb helps in recognisesing web technologies used by a particular website,here’s the tool in action Using grep , I took out only those websites which were built upon PHP cat whatweb-result|grep “PHP” Saved all the PHP websites in one file, used webscreenshot to took screenshot of them and also made bash loop for dirsearch (for bruteforcing directories). for i in `cat target`;do dirsearch -u $i -e *done; From the screenshot, I found a website which was having blank page, I checked the source code and found few php endpoints there.Opened them,but there wasn’t anything.So I checked the dirsearch scan results for this website , interesting there was 200 ok response on /.svn/all-wcprops Quickly opened it and I saw there many php files name with their full path.Like this there were many php endpoints. /var/www/test.php …………../db.php I made a wordlist with all these php endpoints , stripping the path.Again used dirsearch on this website with this newly created wordlist. When I opened one of this php endpoint, it was disclosing the private/public Rsa key.I quickly reported it.I wasn’t sure if it would have get accepted or not as I having a doubt if this website was in the scope or not. It was fixed within 1–2 days they shutdown the website, and turns out that the website was ou of scope but still I was awarded with a bounty of $2500, which is really great of them. For dirbruteforcing I have switched to ffuf now,give it a try you will love it :) Thankyou so much,if you have read it till the end really appreciated your time.Godluck for 2020 :) Sya Everyonee!!!","headline":"Bug Hunting Journey of 2019","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2019/12/31/bug-hunting-journey-of-2019.html"},"url":"http://localhost:4000/2019/12/31/bug-hunting-journey-of-2019.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css">
  <link rel="stylesheet" href="/assets/css/custom-dark.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Your awesome title" />
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Your awesome title</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Bug Hunting Journey of 2019</h1>
    <p class="post-meta"><time class="dt-published" datetime="2019-12-31T00:00:00+05:30" itemprop="datePublished">
        Dec 31, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Heyyy Everyoneee,</p><p>I hope you all are doing good, this year is about to end.</p><p>So I thought I should share a last writeup about some of the bugs which I have found this year.This is going to be a little long.I have been working on this for the last few days ,I hope you will like it.</p><p>I will be starting from the xss bugs which I found in a Private Program websites,I decided to look into the android apps in hope of finding some new endpoints that might be vulneable.</p><h4>Here we go….</h4><p>I never checked their android apps before so I decided to give it a try.I downloaded the android apps from app.evozi.com, after downloading them I used <a href="https://ibotpeaches.github.io/Apktool/">apktool</a> to decompile them.Then I search for http/https endpoints.</p><p>You can use grep to search for strings starting with https,http.</p><p>I was suprised to see some new domains which I didn’t find earlier,tried putting the same vulnerable endpoints in which I have found xss earlier,result successful popup.</p><p>But sad, someone already reported it, 11 days ago.</p><p>You can use this tool for extracting endpoints/URLs from apk files. <a href="https://github.com/s0md3v/Diggy">https://github.com/s0md3v/Diggy</a></p><p><a href="https://github.com/s0md3v/Diggy">s0md3v/Diggy</a></p><p>I didn’t knew about apktool,grep at first so I was doing it all manually. Extracting the apk file , then converting the classes.dex file into jar using dex2jar and then finally searching for http,https</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*_2ZofWEqllCb90WY9jh9pg.png" /></figure><p>By seeing the above screenshot you could easily see that I was doing it the hard way(wrong way).Later on I started looking for an easy way and found <a href="https://github.com/s0md3v">s0md3v</a>’s tool.</p><h4>Now moving on to Verizon Media</h4><p>I won’t be mentioning much more details about the bugs found in VerizonMedia program as they don’t allow disclosure of their bugs,I will be only talk about the process how I came across this bugs.</p><p>I decided to start hunting on Verizon Program, I was just crazy to find a xss in yahoo subdomain.</p><p>Fired up my burpsuite and I collected the subdomains from various subdoman enumeration tools like subfinder,assetfinder,findomain,etc . one by one I was opening the subdomains in my browser,I was monitoring them in burp (I was focused more on finding some strange looking subdomains) and also selecting all the inscope hosts and spidering them in hope of finding some more subdomains, following Jhaddix way (Checkout out his videos, if you haven’t).</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FNUsJpquFq0Q%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DNUsJpquFq0Q&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FNUsJpquFq0Q%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/98ee338878b766e261e8d39047ba76b8/href">https://medium.com/media/98ee338878b766e261e8d39047ba76b8/href</a></iframe><p>I continued following the same routine,well it was so boring and tedious but still I continued looking for a xss.</p><p>One day I found a strange 3 level yahoo subdomain in Target area,I clicked on it all I can see there is only two endpoints but there were many parameters,so I started testing these parameters ,my input got reflected into the source page for eg : ‘“&gt;&lt;shirley&gt;<br>as it is,I used the payload “&gt;&lt;img src=x onerror=alert(document.domain)&gt; to make the xss popup.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/496/1*X1G4Owpuu_riOGJvFd_qeg.png" /></figure><p>Quickly I decided to report it.Later,saw many people getting duplicate of this report, felt sad for them :( .If I had found that xss after 1–2days I would have surely gotten duplicate too.</p><p>I had created a python script by following some blog posts from google about how to open urls from a text file one by one in browser at that time,btw there is any easy to do this ,using this addon <a href="https://github.com/htrinter/Open-Multiple-URLs"><strong>Open Multiple URLs</strong></a> it’s available for both Chrome and Firefox just supply it a list of urls and it will open them in your browser all at once.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*otCwnGWJi5HnHe1g-YFi4Q.gif" /></figure><h4>Moving on to the next bug,</h4><p>I started targetting Facebook just after few days I was done with Yahoo,I have already published an article about that subdomain takeover so there isn’t much left to talk about.</p><p>If you haven’t checked it out already yet,just open the below link,I have talked about in detail how I was able to find this domain:</p><p><a href="https://medium.com/@sudhanshur705/how-recon-helped-me-to-to-find-a-facebook-domain-takeover-58163de0e7d5">How Recon helped me to to find a  Facebook domain takeover</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mztI2tsXNS9pPUCO2QdWsg.png" /></figure><p>I followed the same methodology and was able to find two more subdomain takeovers this time in a Private BugBounty Program.</p><p>Checked the whois records of some company’s owned website,compared the results and then used similar terms to find more domains.Used webscreenshot, then I started going through them to look for interesting stuffs -&gt; found 2 unclaimed github pages.</p><h4>Let’s now talk about some Privilege Escalation Bugs:</h4><p>We can use some automation here with the help of Burp addons like <a href="https://github.com/Quitten/Autorize">Autorize</a>,<a href="https://github.com/PortSwigger/auto-repeater">Autorepeater</a>.You can follow these two links here :</p><p><a href="https://gauravnarwani.com/escalating-privileges-like-a-pro">Escalating Privileges like a Pro</a></p><p>(<a href="https://twitter.com/gauravnarwani97">gauravnarwani97</a>)</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F3K1-a7dnA60%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D3K1-a7dnA60&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F3K1-a7dnA60%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/12821f47199bf52aac3ea925447af0dd/href">https://medium.com/media/12821f47199bf52aac3ea925447af0dd/href</a></iframe><p>Watch this video, it will guide you how to use Autorepeater and Autorize.(<a href="https://twitter.com/stokfredrik">@stokfredrik</a> &amp; <a href="http://Regala_">@Fisher</a>)</p><p>Picked up some private programs from hackerone which have websites that have user management system where I can see they have setup different user roles, so I can check privilege escalation issues.</p><p>For a quick check,I like to start by creating a new account with no permission (absolutely nothing).Then after configuring autorize, with the inscope target.(You can checkout the above mentioned video and articles they include the steps also how to setup autorize for testing privilege escalation issues).</p><p>Login into the high Privilege user account and start going through every endpoint on that web application as much possible, and after some time looking at the autorize tab to see if it has find something.</p><p>The extension autorize automatically repeats every request with the session of the low privileged user and detects authorization vulnerabilities.<br>(<a href="https://portswigger.net/bappstore/f9bbac8c4acf4aefa4d7dc92a991af2f">https://portswigger.net/bappstore/f9bbac8c4acf4aefa4d7dc92a991af2f</a>)</p><p>I was able to found 2 privsec issues in one program and 1 issue in another.One of them which was a low Privilege user was able to view the details of other users in an organization.</p><p>Screenshot of the request and response.(Private Program)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/719/1*uMHjHTTW4p88Jge_SzcxsQ.png" /></figure><p>I really like the website which have already having a check box like table in their doc, so that the users can very easily identify what all permisison a specific type of user is able to perform.</p><p>Well if there isn’t already a table like that, make it yourself it will help you while testing.</p><p>Result:Btw, Got duplicate for all three of them :(</p><h4>Again back to Verizon Media</h4><p>This is going to be about the information disclosure bug,basically I found an endpoint which was disclosing the public/private RSA key just like this</p><blockquote>[privateKey] =&gt; — — — BEGIN RSA PRIVATE KEY — —</blockquote><blockquote>………………………Redacted………………………………</blockquote><blockquote>— — — END RSA PRIVATE KEY — — —</blockquote><p>I was applying the same methodology which I used in Facebook ,in case of Facebook it was about 4k unique domains.But here in case of Verizon Media, there were f**king more than 10k+ domains just from using domain-admin@oath.com as the search term in <a href="https://tools.whoisxmlapi.com/reverse-whois-search">https://tools.whoisxmlapi.com/reverse-whois-search</a></p><p>Another 10k+ domains if I use Oath Inc as the search term.Although I followed the same way like before,collecting all the subdomains -&gt; screenshot.</p><p>Going through them but here if we talk about Yahoo’s site many of them redirects to Yahoo search, Will be right back…,Sorry, the page you requested was not found,etc.It’s going to take a lot of time if I followed the old way.</p><p>I was looking for some other new methodology this time,started reading recon writeups,searching on twitter.</p><p>Got to know about some of the <a href="https://twitter.com/tomnomnom?lang=en">@tomnomnom</a>’s awesome tools-&gt; <a href="https://github.com/tomnomnom/httprobe">httprobe</a>,<a href="https://github.com/tomnomnom/meg">meg</a>,<a href="https://github.com/tomnomnom/assetfinder">assetfinder</a> but still I needed to figure out how should I use them.</p><p>At first I watched</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FDvS_ew77GXA%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DDvS_ew77GXA&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FDvS_ew77GXA%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/580a30542471d90280da28de8895a3c1/href">https://medium.com/media/580a30542471d90280da28de8895a3c1/href</a></iframe><p>where <a href="https://twitter.com/tomnomnom?lang=en">@tomnomnom</a> talked about his recon techniques, he mentioned about how we can use meg to find bugs Path-based xss, Open redirects,CRLF injection,CORS misconfigurations.There were some more videos also of him which I have watched you can find them on his channel and one on <a href="https://www.twitch.tv/nahamsec/videos">Nahamsec</a> stream.</p><p>Like this one which he did with @stokfredrik</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fl8iXMgk2nnY%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dl8iXMgk2nnY&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fl8iXMgk2nnY%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/cbcaa1b1116300945298186858e71de5/href">https://medium.com/media/cbcaa1b1116300945298186858e71de5/href</a></iframe><p>I used httprobe to resolve all the domains which I found from <a href="https://tools.whoisxmlapi.com/reverse-whois-search">https://tools.whoisxmlapi.com/reverse-whois-search</a> and saved the output to a file.</p><blockquote>cat domains|httprobe -c 100|tee hosts</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/830/1*C_MThVwOdQawOpENaQ24IQ.png" /></figure><p>Then with the help of sed ,removed <em>http[s]:// part </em>from the contents of the hosts file and used <em>sort -u</em> command to removed the duplicate ones pipe the output to assetfinder which will find subdomains for us and after .</p><blockquote>cat hosts | sed ‘s/^http\(\|s\):\/\///g’\ | sort -u | assetfinder —subs-only |tee subdomains</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nGnpECAxNa_8Nmr9-QKG2g.png" /></figure><p>After i got the subdomains again used httprobe and after that I used whatweb (got to know about this tool from <a href="https://www.twitch.tv/nahamsec/videos"><strong>Nahamsec </strong></a>stream,<a href="https://twitter.com/infosec_au">Shubham Shah</a> mentioned about it there).</p><p><a href="https://github.com/urbanadventurer/WhatWeb">WhatWeb</a> helps in recognisesing web technologies used by a particular website,here’s the tool in action</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mX-DRc2_PG4OnUS99in_nQ.png" /></figure><p>Using grep , I took out only those websites which were built upon <em>PHP</em></p><blockquote>cat whatweb-result|grep “PHP”</blockquote><p>Saved all the PHP websites in one file, used webscreenshot to took screenshot of them and also made bash loop for dirsearch (for bruteforcing directories).</p><blockquote>for i in `cat target`;do<br> dirsearch -u $i -e *<br>done;</blockquote><p>From the screenshot, I found a website which was having blank page, I checked the source code and found few php endpoints there.Opened them,but there wasn’t anything.So I checked the dirsearch scan results for this website , interesting there was 200 ok response on <strong><em>/.svn/all-wcprops</em></strong></p><p>Quickly opened it and I saw there many php files name with their full path.Like this there were many php endpoints.</p><blockquote>/var/www/test.php</blockquote><blockquote>…………../db.php</blockquote><p>I made a wordlist with all these php endpoints , stripping the path.Again used dirsearch on this website with this newly created wordlist.</p><p>When I opened one of this php endpoint, it was disclosing the private/public Rsa key.I quickly reported it.I wasn’t sure if it would have get accepted or not as I having a doubt if this website was in the scope or not.</p><p>It was fixed within 1–2 days they shutdown the website, and turns out that the website was ou of scope but still I was awarded with a bounty of $2500, which is really great of them.</p><p>For dirbruteforcing I have switched to <a href="https://github.com/ffuf/ffuf">ffuf </a>now,give it a try you will love it :)</p><p>Thankyou so much,if you have read it till the end really appreciated your time.Godluck for 2020 :)</p><p>Sya Everyonee!!!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=95e5190aca7c" width="1" height="1" alt="">

  </div><a class="u-url" href="/2019/12/31/bug-hunting-journey-of-2019.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">GitHub User</li>
          <li><a class="u-email" href="mailto:your-email@domain.com">your-email@domain.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
